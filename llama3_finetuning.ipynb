{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: OpenAI failed to import - ignoring for now.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.2.0. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 2.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.24. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Optional with limited VRAM\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # load_in_4bit = load_in_4bit,\n",
    "    token = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'most_similar_instructions', 'avg_similarity_score'],\n",
       "    num_rows: 10110\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"/path_to_train_dataset\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9110/9110 [01:02<00:00, 145.78 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:08<00:00, 117.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CUTOFF_LEN = 2048\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Create the text prompt from your instruction, input, and output fields.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    \"\"\"\n",
    "    Tokenizes the prompt. Optionally pads to max_length=2048 and appends an EOS token.\n",
    "    Copies input_ids to labels for causal LM.\n",
    "    \"\"\"\n",
    "    # Here, we use padding=\"max_length\" to get uniform-length sequences of 2048.\n",
    "    # Alternatively, you can use padding=False and rely on a data collator.\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",   # or padding=False + data_collator\n",
    "        return_tensors=None,    # return raw Python lists\n",
    "    )\n",
    "\n",
    "    input_ids = result[\"input_ids\"]\n",
    "    attention_mask = result[\"attention_mask\"]\n",
    "\n",
    "    # Optionally place an EOS token at the very end if there's room\n",
    "    if (\n",
    "        add_eos_token\n",
    "        and len(input_ids) == CUTOFF_LEN\n",
    "        and input_ids[-1] != tokenizer.eos_token_id\n",
    "    ):\n",
    "        # Replace last token with EOS if you'd like\n",
    "        input_ids[-1] = tokenizer.eos_token_id\n",
    "        attention_mask[-1] = 1\n",
    "\n",
    "    labels = input_ids.copy()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Combines prompt generation with tokenization.\n",
    "    \"\"\"\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    return tokenize(full_prompt)\n",
    "\n",
    "# Example: split the \"train\" set into train/val\n",
    "train_val = data[\"train\"].train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "train_data = train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    "val_data   = train_val[\"test\"].map(generate_and_tokenize_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = val_data,\n",
    "    dataset_text_field = \"output\",\n",
    "    logging_steps = 200,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        # warmup_steps = 5,\n",
    "        warmup_ratio = 0.05,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 5000,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_32bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"/finetuned_model\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.79s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer\n",
    ")\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model) # make sure to check if the models are correct!\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the AS paths for each prefix associated with ASN AS4766 over the period oct 28 13:00 to oct 28 13:15, 2024. Provide minimum, maximum, and median AS path lengths and highlight any significant path changes observed in BGP updates. \n",
      "# Import necessary libraries\n",
      "import pybgpstream\n",
      "import statistics\n",
      "\n",
      "# Define the time window\n",
      "start_time = \"2024-10-28 13:00:00\"\n",
      "end_time = \"2024-10-28 13:15:00\"\n",
      "\n",
      "# Initialize the BGPStream\n",
      "stream = pybgpstream.BGPStream(\n",
      "    from_time=start_time,\n",
      "    until_time=end_time,\n",
      "    record_type=\"updates\",\n",
      "    filter=\"peer AS4766\"\n",
      ")\n",
      "\n",
      "# Dictionary to store AS paths for each prefix\n",
      "prefix_as_paths = {}\n",
      "\n",
      "# Process BGP records\n",
      "for rec in stream.records():\n",
      "    for elem in rec:\n",
      "        if 'as-path' in elem.fields:\n",
      "            prefix = elem.fields['prefix']\n",
      "            as_path = elem.fields['as-path'].split(' ')\n",
      "            as_path_length = len(as_path)\n",
      "            if prefix not in prefix_as_paths:\n",
      "                prefix_as_paths[prefix] = []\n",
      "            prefix_as_paths[prefix].append(as_path_length)\n",
      "\n",
      "# Calculate minimum, maximum, and median AS path lengths for each prefix\n",
      "for prefix, as_path_lengths in prefix_as_paths.items():\n",
      "    min_length = min(as_path_lengths)\n",
      "    max_length = max(as_path_lengths)\n",
      "    median_length = statistics.median(as_path_lengths)\n",
      "    print(f\"Prefix: {prefix}, Min AS Path Length: {min_length}, Max AS Path Length: {max_length}, Median AS Path Length: {median_length}\")\n",
      "\n",
      "# Highlight any significant path changes\n",
      "for prefix, as_path_lengths in prefix_as_paths.items():\n",
      "    if len(set(as_path_lengths)) > 1:\n",
      "        print(f\"Significant path change detected for prefix {prefix}: {as_path_lengths}\")  # Print all unique AS path lengths for the prefix. Adjust the print statement as needed to highlight the significant change. \n",
      "    else:\n",
      "        print(f\"No significant path change detected for prefix {prefix}\")  # Adjust the print statement as needed to indicate no change. \n",
      "``` \n",
      "The code is a Python script using the PyBGPStream library to collect BGP update data for a specified time window. It filters the data for updates from ASN AS4766 and calculates the minimum, maximum, and median AS path lengths for each associated prefix. It also highlights any significant path changes by identifying prefixes with more than one unique AS path length. \n",
      "\n",
      "Here's how to use the code:\n",
      "1. Set the time window to the desired period (Oct 28 13:00 to Oct 28 13:15, 2024).\n",
      "2. Filter the data for updates from ASN AS4766.\n",
      "3. Calculate the minimum, maximum, and median AS path lengths for each prefix.\n",
      "4. Highlight any significant path changes by identifying prefixes with more than one unique AS path length.\n",
      "\n",
      "The output of the code will be a summary of the AS path lengths for each prefix, including minimum, maximum, and median values, as well as any significant path changes detected. \n",
      "The code can be modified to adjust the output format, add more analysis, or use different parameters to suit specific needs. \n",
      "For example, you can modify the filter to collect data from a specific peer ASN, or add more analysis to track the changes in AS path lengths over time. \n",
      "The code can also be extended to use other libraries or tools for data analysis and visualization. \n",
      "The output of the code can be used to identify trends, anomalies, or other insights related to the AS path lengths and significant path changes. \n",
      "This can be useful for network configuration and reliability analysis, AS path inflation detection, or other network-related tasks. \n",
      "The code can be used as a starting point for more complex analysis or to develop a more comprehensive tool for network monitoring and analysis. \n",
      "The output of the code can be saved to a file or database for later analysis or to track changes over time. \n",
      "The code can be run in batch mode to collect data for a longer period or to analyze multiple sets of data. \n",
      "The code can be modified to use a different data source, such as a live BGP feed or a historical BGP archive. \n",
      "The code can be extended to analyze other BGP attributes, such as community strings or policy information. \n",
      "The code can be used to detect anomalies or irregularities in the AS path lengths, which can indicate potential security threats or network issues. \n",
      "The code can be integrated with other tools or systems for more comprehensive network monitoring and analysis. \n",
      "The code can be used to generate alerts or notifications for significant path changes or other anomalies detected in the AS path lengths. \n",
      "The code can be modified to use a more efficient data structure or algorithm to improve performance for large datasets. \n",
      "The code can be extended to analyze\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1012)\n",
    "result = pipe(f\"Summarize the AS paths for each prefix associated with ASN AS4766 over the period oct 28 13:00 to oct 28 13:15, 2024. Provide minimum, maximum, and median AS path lengths and highlight any significant path changes observed in BGP updates.\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('your_hf_acc/repo_name')\n",
    "tokenizer.push_to_hub('your_hf_acc/repo_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
